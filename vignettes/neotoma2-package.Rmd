---
title: "The neotoma2 R Package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{The neotoma2 R Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(neotoma2)
library(sf)
```

## Neotoma Data Structure

Data in Neotoma is associated with sites, specific locations with lat/long coordinates. Within a site, there may be one or more **collection units** -- locations at which samples are physically collected within the site. For example, an archaeological **site** may have one or more **collection units**, pits within a broader dig site; a pollen sampling **site** on a lake may have multiple **collection units** -- core sites within the lake basin. Collection units may have higher resolution GPS locations, but are considered to be part of the broader site. Within a **collection unit** data is collected at various **analysis units** from which **samples** are obtained.

Because Neotoma is made up of a number of constituent databases (e.g., the Indo-Pacific Pollen Database, NANODe, FAUNMAP), a set of **sample**s associated with a **collection unit** are assigned to a single **dataset** associated with a particular **dataset type** and **constituent database**.

Researchers often begin by searching for sites within a particular study area, whether that is defined by geographic or political boundaries.  From there they interrogate the available datasets for their particular dataset type of interest.  When they find records of interest, they will then often call for the data and associated chronologies.

The `neotoma2` R package is intended to act as the intermediary to support these research activities using the Neotoma Paleoecology Database.

### Package Requirements

The `neotoma2` package draws primarily on `dplyr` and `purrr` packages from the `tidyverse`, and on the `sf` spatial data package.

## Site Searches

Sites can be searched using the `get_sites()` function, or, can be created using the `set_site()` function. A single `site` object is a special object in R, that can be combined into a `sites` object.  A `sites` object is effectively a `list()` of `site` objects with special methods.

### Finding sites

You can find a site by its unique numeric Neotoma site id (`siteid`), by name (`sitename`), by altitude (`altmin`, `altmax`), by geopolitical name (`gpid`), or by location (`loc`).

#### Using a Site ID

If we're looking for a site and we know its specific site identifier, we can use the simplest implementation of `get_sites()`.  Here we are searching for a site (Alexander Lake), where we know that the siteid for the record in Neotoma is `24`:

```{r getSiteBySiteID}
alex <- get_sites(24)
alex
```

#### Using Multiple Site IDs
It might be of interest to pass a list of sites. You can do so using `c()`

```{r get-multipe-sites}
multiple_sites <- get_sites(c(24, 47))
multiple_sites
```


#### Searching for sites by name

If we're looking for a site and we know its name or a part of its name, we can search using the function with the sitename argument, `get_site(sitename = 'XXX')`, where `'XXX'` is the site name.

```{r getsitename}
alex <- get_sites(sitename = "Alexander Lake")
alex
```

We can also access the different parameters for each site using S4 class marker `@`.

Since the `sites` object is composed by smaller objects of class `site`, we do need to specify which ordered object we want to access. We do so with `[[ ]]` and place the index of the desired object.

```{r extractElement}
alex@sites[[1]]@siteid
```

#### Using approximate site names

Neotoma uses the a Postgres Database to manage data.  Postgres uses the `%` sign as a general wildcard, so we can use the `%` in the `sitename` argument operator to help us find sites when we're not sure the exact match.  Note that the search is case **insensitive** so a search for `alex%` or `Alex%` will return the same results.

````{r getsitename placeholder}
alex <- get_sites(sitename = 'Alexander%')
```

Since this new `sites` object has `r length(alex@sites)` elements that belong to `site`, we can use S4 accessing techniques to gather the data for either the first or second object.

#### Accessing get_sites documentation 

If we ever need to see the documentation for examples, we can always access the help file using `?`

```
?get_sites
```

### Creating a Site

As explained above, a `site` is the fundamental unit of the Neotoma Database.  If you are working with your own data, you might want to create a `site` object to allow it to interact with other data within Neotoma.  You can create a site with the `set_site()` function. It will ask you to provide important information such as `sitename`, `lat`, and `long` attributes.

```{r set_site}
my_site <- set_site(sitename="My Lake", 
                    coordinates = st_sf(a=3, st_sfc(st_point(1:2))), 
                    description = "my lake", 
                    altitude = 30)
my_site
```

#### Accessing `set_site()` documentation 

If we ever need to see the documentation for examples, we can always access the help file using `?`

```
?set_site
```

## Datasets

If you need to get to a deeper level of the sites object, you may want to look at the `get_datasets()` function.

With get_datasets, we make calls to the API to get each dataset information -DatasetID, location, datasettype and so on.

Getting the datasets by id is the easiest call, you can also pass a vector of IDs or if you already have a `sites` object, you can pass a sites object. Be careful, get_datasets does not take `sitename` or `siteid` as inputs.

```{r eval = TRUE}
# Getting datasets by ID
my_datasets <- get_datasets(c(5, 10, 15, 20))
my_datasets
```
You can also retrieve datasets by type directly from the API.

```{r eval = TRUE}
# Getting datasets by type
my_pollen_datasets <- get_datasets(datasettype = "pollen")
my_pollen_datasets
```
When the API has many responses, you can limit the quantity of sites by using the parameter `limit` or you can do a `while`/'for' loop with the parameter `offset` - which works like a pager.

```{r eval = TRUE}
# Getting just 5 pollen datasets
my_5_pollen_datasets <- get_datasets(datasettype = "pollen", limit = 5)
my_5_pollen_datasets
```
```{r eval = TRUE}
# Taking a look at the next 5
offset_param <- c(1, 2, 3)

my_offset_datasets <- c()
for(i in offset_param){
  my_pollen_datasets <- get_datasets(type = "pollen", limit = 5, offset = offset_param[[i]])
  my_offset_datasets <- c(my_offset_datasets, my_pollen_datasets)
}

my_offset_datasets
```
You can get the coordinates to create a GeoJson bounding box from [here](https://geojson.io/#map=2/20.0/0.0)

Accessing datasets by bounding box:

```{r bounding box}
brazil <- '{"type": "Polygon", 
"coordinates": [[
[-73.125, -9.102096738726443],
[-56.953125,-33.137551192346145],
[-36.5625,-7.710991655433217],
[-68.203125,13.923403897723347],
[-73.125,-9.102096738726443]
]]}'

brazil[1]

brazil_datasets <- get_datasets(loc = brazil[1])
brazil_datasets
```

You can plot these findings!

```{r}
plotLeaflet(brazil_datasets)
```

```{r}
filter(brazil_datasets, datasettype == "geochronologic")
```


The filter function takes as the first argument, a datasets object, followed by the critieria we want to use to filter. Current supported criteria includes:

* `lat`
* `long`
* `elev`
* `datasettype`

You also need to make sure that you accompany any of these terms with the following boolean operators: `<`, `>` or `==`, `!=`. Datasettype has to be of type string, while the other terms must be numeric.

If you need to filter by the same argument, let's say, you need to filter "geochronologic" and "pollen datatypes, then you will also make use of & and |. For example:

```{r}
filter(brazil_datasets, datasettype == "geochronologic" | datasettype == "pollen")
```

```{r}
filter(brazil_datasets, lat > -18 & lat < -16)
```

You can also use the pipeline operator for these tasks:
```{r}
brazil_datasets %>%
  filter(lat > -18 & lat < -16)
```


```{r}
Names_744BH<-c("Bald Hill", "Blake (sutton)", "High (sudbry)", "Holland", "Little Hosmer", "Long (grnsbo)", "Long (shefld)", "Round (shefld)")
new_sites <- c()
for(Number in Names_744BH){
  sites <- get_sites(sitename=Number)
  new_sites <- c(new_sites, sites)}
new_sites
```

## The filter Function

Once you have made a call to the API and have the datasets that you want to work with, you can filter the object.

For example, in the case of the Brazil datasets, we might just be interested in the "geochronologic" types of datasets.

Rather than making a new API call, we can filter them using the `filter` function:




## Publications

Many Neotoma records have publications associated with them.  The `publication` object (and the `publications` collection) provide the opportunity to do this.  The [`publication`](http://open.neotomadb.org/dbschema/tables/publications.html) table in Neotoma contains an extensive number of fields.  The methods for `publications` in the `neotoma2` package provide us with tools to retrieve publication data from Neotoma, to set and manipulate publication data locally, and to retrieve publication data from external sources (e.g., using a DOI).

### `get_publications()` from Neotoma

Publications in Neotoma can be discovered using a number of parameters including   

The most simple case is a search for a publication based on one or more publication IDs.  Most people do not know the unique publication ID of individual articles, but this provides a simple method to highlight the way Neotoma retrieves and presents publication information.

#### Get Publication By ID

We can use a single publication ID or multiple IDs.  In either case the API returns the publication(s) and creates a new `publications` object (which consists of multiple individual `publication`s).  

```{r}
one <- get_publications(12)
two <- get_publications(c(12,14))
```

From there we can then then subset and extract elements from the list using the standard `[[` format.  For example:

```{r}
two[[2]]
```

Will return the second publication in the list, corresponding to the publication with `publicationid` 14 in this case.

#### Get Publication using Search:

We can also use search elements to search for publications.  The `get_publication` method uses the Neotoma API to search for publications.  We can search using the following properties:

* `publicationid`
* `datasetid`
* `siteid`
* `familyname`
* `pubtype`
* `year`
* `search`
* `limit`
* `offset`

```{r}
michPubs <- get_publications(search="Michigan", limit=2)
length(michPubs)
```

This results in a set of `r length(michPubs)` publications from Neotoma.  Equal to the `limit` in this case.  If the number of matching publications is less than the limit then the `length()` will be smaller.

Text matching in Neotoma is approximate, meaning it is a measure of the overall similarity between the search string and the set of article titles.  This means that using a nonsense string may still return results results:

```{r}
noise <- get_publications(search="Canada Banada Nanada", limit = 5)
```

This returns a result set of length `r length(noise)`.  

By default the `limit` for all queries is `25`.  The default `offset` is `0`.  To capture all results it is possible to add `limit=99999999` or some similarly high number.  **However**, this is hard on the Neotoma servers.  Best practice is to loop through results, using `limit` and `offset`, for example, in a `while` loop.  In a result set of 100 records, the `limit`, when `offset` is 0 (the default), ensures that only the first 25 records are returned.  Keeping the `limit` at 25, and increasing the `offset` to 25 would give us the next 25 records.

We can use that in a `while` loop in R in the following way:

```{r}
run = TRUE
offset <- 0

while(run) {
  newPubs <- get_publications(search="Michigan", offset=offset)
  if(length(newPubs) == 0) {
    run = FALSE
  }
  if(exists('allPubs')) {
    allPubs <- c(allPubs, newPubs)
  } else {
    allPubs <- newPubs
  }
  offset <- offset + 25
}
```

This gives us an object of length `r length(allPubs)`, the number of publications in Neotoma that reference Michigan.

We can see a summary of the returned object by simply calling it:

```{r}
one
```

This returns the (Neotoma) ID, the citation and the publication DOI (if that is stored in Neotoma).  We can get the first publication using the standard `[[` nomenclature:

```{r}
two[[1]]
```

The output will look similar to the output for `two` above, however you will see that only a single result will be returned and the class (for a single publication) will be of type `publication` (as opposed to `publications`).  

We can select an array of `publication` objects using the `[[` method, either as a sequence (`1:10`, or as a numeric vector (`c(1,2,3)`)):

```{r subsetPubs}
# Select publications with Neotoma Publication IDs 1 - 10.
pubArray <- get_publications(1:10)
# Select the first five publications:
subPub <- pubArray[[1:5]]
subPub
```

#### Create (or Import) New Publications

Just as we can use the `set_sites()` function to set new site information, we can also create new publication information using `set_publications()`.  With `set_publications()` you can enter as much or as little of the article metadata as you'd like, but it's designed (in part) to use the CrossRef API to return information from a DOI.

```{r setNewPub}
new_pub <- set_publications(articletitle = 'Myrtle Lake: a late- and post-glacial pollen diagram from northern Minnesota',
                           journal = 'Canadian Journal of Botany',
                           volume = 46)
```

A `publication` has a large number of slots that can be defined.  These may be left blank, they may be set directly after the publication is defined:

```{r setPubValue}
new_pub@pages <- '1397-1410'
```


#### Validate New Publications


## Workflows

One of the main goals of Neotoma is to pull records out of Neotoma into the R workflow.  

### Diatoms

```{r getDiatoms, eval=FALSE}
neDiatoms <- get_datasets(datasettype="diatom surface samples", loc=BOUNDING BOX)

neDiatoms <- get_datasets(dois=c(...))
```

* Pull diatom surface samples.
  * We may want to pull spatially, or pull by "National Assessment", or by other defined calibration set.

```{r getWaterChem, eval=FALSE}
neDiatoms <- get_datasets(datasettype="diatom surface samples", loc=BOUNDING BOX)
```

* Look for water chemistry at the same site (or collection unit).

Look for a table that explicitly matches water chemistry and diatom datasets, in cases where records have been sampled multiple times.

* In theory collection dates are a good idea to use to filter, but collection dates might not be specific enough.

* In some regional studies water chemistry was sampled multiple times during the summer (e.g.) but diatoms were sampled only a single time.

* Was this going to be done in the aggregate dataset method?  Didn't happen.

Then:
* With one diatom record per water chemistry record
* we need taxon name, we need water chemistry type & units

Then we do some inference modeling (`rioja`).  Building e.g., transfer functions.

* Then run predictions on cores & plot predicted values & error

* We would extract an appendix of publications for the datasets we use.
* We would want some maps of site locations
* We would want some simple scatter plots
* We would want to do something about understanding taxonomic differences (so we'd want to know who the analyst was, or who the PI was)
* We might want to group or synonomize.  Can we pull those from Neotoma?
* We want the stratigraphic diagrams for cores, and surface sample diagrams for surface sample sets
* We want the lists of dataset DOIs.
* Want abundances of taxa along a gradient (with superimposed curves?)
